This thesis explores the use of Bayesian models in multi-player video game AI, particularly real-time strategy (RTS) game AI. Video games are in-between real world robotics and total simulations, as other players are not simulated, nor do we have control over the simulation. RTS games require having strategic (technological, economical), tactical (spatial, temporal) and reactive (units control) actions and decisions on the go. %%%This decisions, made up from incomplete/partial information, also constrains each others. As the actions of players are real-time and simultaneous, dynamic behavior adaptation is key to expert human playing.
We used Bayesian modeling as an alternative to logic, able to cope with incompleteness of information and uncertainty. %%%Our claim is that, through pragmatic Bayesian modeling and machine learning, game AI programmers can achieve more adaptable and robust behaviors. %way more efficiently than by scripting (full specification of the behavior) or planning (full specification of the search problem) approaches. 
Indeed, incomplete specification of the possible behaviors in scripting, or incomplete specification of the possible states in planning/search raise the need to deal with uncertainty. Machine learning helps reducing the complexity of fully specifying such models. 
Through the realization of a fully robotic StarCraft player, we show that Bayesian programming can integrate all kinds of sources of uncertainty (hidden state, intention, stochasticity). Probability distributions are a mean to convey the full extent of the information we have %%%on a problem 
and can represent by turns: constraints, partial knowledge, state estimation, and incompleteness in the model itself.

In the first part of this thesis, we review the current solutions to problems raised by multi-player game AI, by outlining the types of computational and cognitive complexities in the main gameplay types. From here, we sum up the cross-cutting categories of problems, explaining how Bayesian modeling can deal with all of them. We then explain how to build a Bayesian program from domain knowledge and observations through a toy role-playing game example. In the second part of the thesis, we detail our application of this approach to RTS AI, and the models that we built up. For reactive behavior (micro-management), we present a real-time multi-agent decentralized controller inspired from sensorimotor fusion. We then show how to perform strategic and tactical adaptation to a dynamic opponent through opponent modeling and machine learning (both supervised and unsupervised) from highly skilled players' traces. These probabilistic player-based models can be applied both to the opponent for prediction, or to ourselves for decision-making, through different inputs. Finally, we explain our StarCraft robotic player architecture and precise some technical implementation details.

Beyond models and their implementations, our contributions fall in two categories: integrating hierarchical and sequential modeling and using machine learning to produce or make use of abstractions. We dealt with the inherent complexity of real-time multi-player games by using a hierarchy of constraining abstractions and temporally constrained models. We produced some of these abstractions through clustering; while others are produced from heuristics, whose outputs are integrated in the Bayesian model through supervised learning.

