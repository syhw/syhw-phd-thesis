\chapter{Conclusion}
\begin{quotation}\textit{
Man's last mind paused before fusion, looking over a space that included nothing but the dregs of one last dark star and nothing besides but incredibly thin matter, agitated randomly by the tag ends of heat wearing out, asymptotically, to the absolute zero.\\
Man said, ``AC, is this the end? Can this chaos not be reversed into the Universe once more? Can that not be done?''\\
AC said, ``THERE IS AS YET INSUFFICIENT DATA FOR A MEANINGFUL ANSWER.''
}
\begin{flushright}Isaac Asimov (The Last Question, 1956)\end{flushright}\end{quotation}

\lettrine{I}{ntro}
\ifthenelse{\equal{\myebookformat}{false}}{
\chaptertoc
}{}

\section{Perspectives}
\label{chapter:perspectives}
\subsection{Micro}

Reinforcement learning, \citep{Marthi05concurrenthierarchical}


Evolving control policies, \citep{Miles2007}
%\citep{Schoenauer1996} 
\subsection{Tactics}

Two important research directions for tactics would be:
\begin{itemize}
    \item Tactical assault generation, so that we do not have to hard-code the tactical goals behaviors. Previous works, which would be compatible with StarCraft, in this direction include \citep{Chung05,PonsenMSA06,UCT,GellySchoenauer}.
    \item Improve tactical state estimation, so that both our tactical decision-making and tactical prediction benefit it. A first step would be to use a (dynamic) filtering on enemy units, either with a particle filter \citep{Thrun02d,weber2011aiide} or with hidden semi-Markov models \citep{Hladky_anevaluation}. We propose here a simple units filtering model based on the decomposition of the map in regions.
\end{itemize}


\subsection{Strategy}
Bandits (contextual bands)\\
Planning \citep{Wolfe11}\\
Probabilistic planning?

\subsection{Inter-game Adaptation (Meta-game)}

\glos{metagame} % TODO
Other cases of learning not dealt with in previous chapters:
\citep{metalevelbehavioradaptrts}
Laplace smoothing $P(ETT=ett|Player=p)= \frac{1 + nbgames(ett,p)}{\#ETT + nbgames(p)}$. Same for $EClusters$ and $ETactics$.\\

The full (reinforcement) learning of the bot can be seen as learning degrees of liberty one by one with a hierarchy from strategies to actions by tactics. C.f R-IAC Baranes \& Oudeyer 2009 and 2012; $PI^2$-CMA (ES).\\

meta question I think that he thinks that I think that...
\citep{RussellW89}
\citep{metaMCTS}

\section{Contrib}
RÃ©sumer les contributions
\subsection{Approaches}
%%% \begin{itemize}
%%% \item train your IA from data
%%% \item or train your IA by itself
%%% \item or let the game designers set the parameters
%%% \end{itemize}
\begin{itemize}
\item A tractable decomposition of game AI in a hierarchy of predictions, decisions and actions.
\item Integration of learning in a decision-making model. 
\item An autonomous agent for StarCraft (BroodwarBotQ).
\end{itemize}

\subsection{Results}
Recall what works, what should be extended.

\section{Perspectives: Not a solved problem yet}
Computers don't beat good (experts) humans (higher level strategic thinking: common sense, plus vision/interpolation for efficient micro). They are not so fun (do not adapt that much, our bot is the most adaptive ATM). Competition results.
Tout ce qui peut se faire en recherche et ce qui est directement applicable par l'industrie.


