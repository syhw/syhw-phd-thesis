\chapter{Conclusion}
\begin{quotation}\textit{
Man's last mind paused before fusion, looking over a space that included nothing but the dregs of one last dark star and nothing besides but incredibly thin matter, agitated randomly by the tag ends of heat wearing out, asymptotically, to the absolute zero.\\
Man said, ``AC, is this the end? Can this chaos not be reversed into the Universe once more? Can that not be done?''\\
AC said, ``THERE IS AS YET INSUFFICIENT DATA FOR A MEANINGFUL ANSWER.''
}
\begin{flushright}Isaac Asimov (The Last Question, 1956)\end{flushright}\end{quotation}

%%% \ifthenelse{\equal{\myebookformat}{false}}{
%%% \chaptertoc
%%% }{}

%\section{Summary}
\section{Contributions summary}
We classified the problems raised by game AI, and in particular by RTS AI. We showed how the complexity of video games makes it so that game AI systems can only be incompletely specified. This leads to uncertainty about our model, but also about the model of the opponent. Additionally, video games are often partially observable, sometimes stochastic, and most of them require motor skills, which will introduce randomness in the outcomes of player's actions. We chose to deal with incompleteness by transforming it into uncertainty about our reasoning model. We bind all these sources of uncertainty in Bayesian models. 

%Specifically, we attacked three problems at three abstraction levels:
Our contributions about breaking the complexity of \textit{specifying} and \textit{controlling} game AI systems are resumed by:
\begin{itemize}
    \item In chapter~\ref{chapter:micro}, we produced reactive, decentralized, multi-agent control by transforming the incompleteness about allied units intentions into uncertainty of their future locations. This can be viewed as an extension of Bayesian robot programming \citep{Lebeltel04} in a multi-agent setting. Instead of specifying a distribution on the possible directions knowing the sensory inputs, we specified the sensor distribution (independently of each other sensors) knowing the direction ($\PP(Sensor|Direction)$). This approach, called \textit{inverse programming}, can be viewed as ``instead of specifying the states and their transitions based on sensors (an FSM), we specifying what the sensors should be when we are in a given state'', reverting some of the burden of specifying behaviors, and in a probabilistic setting. This is an extension of the work on inverse programming for Unreal Tournament avatars \citep{LeHy04}. Combined, these contributions lead to real-time micro-management behaviors for StarCraft, which achieved very satisfying results (ex-aequo with the best micro-management AI of AIIDE 2010 competition), and were published in \citep{SYNNAEVE:Micro}.

    \item By going up in the ladder of abstraction (strategy \& tactics), we were able to exploit what we previously called \textit{vertical continuity} (see section~\ref{sec:verticalcontinuity}) through hierarchical models. About strategy (chapter~\ref{chapter:strategy}), from low-level observations, we produced \gloss{buildtree} (section~\ref{sec:techtreepred}, \citep{SYNNAEVE:StratPred}), and built upon that to infer \gloss{opening} (section~\ref{sec:openingspred}, \citep{SYNNAEVE:OpeningPred}) and to constrain the inference on the opponent's army composition (section~\ref{sec:armycomposition}). Tactics (chapter~\ref{chapter:tactics}) also make good use of the prediction on the opponent's \glos{techtree}. When used in decision-making, our models are even more constrained, because we have full knowledge of our state instead of distributions.

    \item We also took advantage of actions sequencing, previously called \textit{horizontal continuity} (see section~\ref{sec:horizontalcontinuity}), by assessing that, often, things which are done should not be undone (at least not immediately) and that some strategic and tactical steps are prerequisite of intended steps. At the strategic level, the distribution on \gloss{buildtree} is time-dependent: the sequencing is encoded in the learned discrete Gaussian distributions (section~\ref{sec:techtreepred}, \citep{SYNNAEVE:StratPred}). The openings are filtered on previous inferences with a first order Markovian assumption (section~\ref{sec:openingspred}, \citep{SYNNAEVE:OpeningPred}), i.e. the value at time $t$ is dependent on the value at time $t-1$. The army composition model (section~\ref{sec:armycomposition}) makes use of temporal continuity to adapt the player's army to the opponent's future army composition. %Tactics are based on lower-level units movement tracking (filtering). 
Micro-management assumes an uncertain linear interpolation of future units position resulting of trajectory continuity (instead of considering all possible positions), as explained in subsection~\ref{sec:microrepulsionattraction}.
\end{itemize}

We used machine learning to help specifying our models, both used for prediction (opponent modeling) and for decision-making. We exploited different datasets for mainly two separate objectives:
\begin{itemize}
    \item We produced relevant \textit{abstract} models thanks to learning. For the labeling replays (allowing for supervised learning of our full \gloss{opening} prediction model), we used semi-supervised (by selecting features and a scoring function) \glos{GMM} clustering of replays, as presented in section~\ref{sec:replayslabeling} \citep{SYNNAEVE:OpeningPred}. In order to reason qualitatively and quantitatively about armies composition with a tractable model, we applied GMM to armies unit types percentages to find the different composing components (section~\ref{sec:armycomposition}). For tactics, we used heuristics (see subsection~\ref{sec:scoringheuristics}) for the evaluation of the regions, whose bias (incompleteness) the model was adapted to by learning.
    
    \item We learned the \textit{parameters} of our models from human-played games datasets. At the strategic level, we learned the time-dependent distributions on the build trees and the co-occurrence of openings with build trees in sections~\ref{sec:techtreepred} and \ref{sec:openingspred}, respectively published as \citep{SYNNAEVE:StratPred}, \citep{SYNNAEVE:OpeningPred}. We were also able to study the strengths and weaknesses of openings this way (subsection~\ref{sec:openingsstrengthsweaknesses}, and we looked at the dynamics or army compositions (section~\ref{sec:armycomposition}). For tactics (see chapter~\ref{chapter:tactics}), we learned the co-occurrences of attacks with regions tactical properties.
\end{itemize}

Finally, we produced a StarCraft bot (chapter~\ref{chapter:bot}), which ranked 9th (on 13) and 4th (on 10), respectively at the AIIDE 2011 and CIG 2011 competitions. It is about 8th (on $\approx$ 20) on an independent ladder containing all bots submitted to competitions (see Fig.~\ref{fig:ladderbots}).

%%% \begin{itemize}
%%% \item train your IA from data
%%% \item or train your IA by itself
%%% \item or let the game designers set the parameters
%%% \end{itemize}

%%% \begin{itemize}
%%% \item A tractable decomposition of game AI in a hierarchy of predictions, decisions and actions.
%%% \item Integration of learning in a decision-making model. 
%%% \item An autonomous agent for StarCraft (BroodwarBotQ).
%%% \end{itemize}

\section{Perspectives}
\label{chapter:perspectives}
\subsection{Micro}
\subsubsection{Reactive behaviors}
An improvement (explained in subsection~\ref{sec:microreducingcollisions}) over our existing model usage would consist in using the distributions on directions ($\PP(Dir)$) for each units to make a centralized decision about which units should go where. This would allow for coordinated movements while retaining the tractability of a decentralized model: the cost for units to compute their distributions on directions ($\PP(Dir)$) is the same as in the current model, and there are methods to select the movements for each unit which are linear in the number of units (for instance maximizing the probability for the group, i.e. for the sum of the movements).

For the problem of avoiding local optima ``trapping'', we proposed a ``trailing pheromones repulsion'' approach in subsection~\ref{sec:microavoidlocal} (see Fig.~\ref{fig:BayesianTrailingPheromone}), but other (adaptive) pathfinding approaches can be considered.

\subsubsection{Parameters identifications}

Furthermore, the identification of the probability distributions of the sensors knowing the directions ($\PP(Sensor|Direction)$) is the main point of possible improvements. In the industry, behavior could be authored by game designers equipped with an appropriate interface (with ``sliders'') to the model's parameters. As a competitive approach, reinforcement leaning or evolutionary learning of the probability tables (or probability distributions' parameters) seems the best choice. The two main problems are: 
\begin{itemize}
    \item types and/or levels of opponents: as we cannot assume optimal play from the opponents (at least not for large scale battles), the styles and types of the opponents' control will matter for the learning.
    \item differences in situations: as there are several types of micro-management situations, we have to choose the granularity of learning settings (battles) and how we recognize them in-game. We could consider the continuum of situations, and use Bayesian fusion of posteriors from models %parameters 
learned in discrete contexts.
\end{itemize}
As the domain (StarCraft) is large, distributions have to be efficiently parametrized (normal, log-normal, exponential distributions should fit our problem). The two main approaches to learn these sensors distributions would be:
\begin{itemize}
    \item Concurrent hierarchical reinforcement learning (\citep{Marthi05concurrenthierarchical} showed how it can work in Wargus).
        \item Co-evolving control policies by playing them against each others (\citep{Miles2007,Avery09} presented a related work with influence maps).
\end{itemize}.
%\citep{Schoenauer1996} 

\subsection{Tactics}

We explained the possible improvements around our model in subsection~\ref{sec:tacticspossibleimprovements}. The three most interesting research directions for tactics would be:
\begin{itemize}
    %\item Improve tactical state estimation, so that both our tactical decision-making and tactical prediction benefit it. A first step would be to use a (dynamic) filtering on enemy units, either with a particle filter \citep{Thrun02d} as \cite{weber2011aiide} did in Starcraft, or with hidden semi-Markov models \citep{Hladky_anevaluation}. We proposed here a simpler units filtering model based on the decomposition of the map in regions in section~\ref{sec:enemyunitsfilter}.
    \item Improve tactical state estimation, so that both our tactical decision-making and tactical prediction benefit from it. A first step would be to use a (dynamic) filtering on enemy units. We proposed a simpler units filtering model based on the decomposition of the map in regions in section~\ref{sec:enemyunitsfilter}.

    \item Use our learned parameters as bootstrap (``prior'') and keep on learning against a given opponent and/or on a given map. We should count how often and in which circumstances an attack, which should be successful, fails. It should even be done during a given game (as human players do). This may be seen as an exploration-exploitation trade-off in which our robotic player wants to minimize its regret for which multi-armed bandits \citep{Kuleshov2000} are a good fit.%, and have proved their valor in Go AI \citep{GellySchoenauer}.

    \item Tactical assault generation, so that we do not have to hard-code the tactical goals behaviors. The definition of \textit{tactics} used in \citep{PonsenMSA06} is not exactly matching ours, but they evolved some strategic and tactical decision elements (evolving knowledge bases for \glos{CBR}) in Wargus. However, we are still far from script-independent tactics (in StarCraft). Being able to infer the necessary steps to carry out a $Drop$ (request a Dropship and military units, put them in the Dropship at some location A and drop them at location B to attack location C, retreat if necessary) attack would be a good benchmark for tactics generation.
\end{itemize}


\subsection{Strategy}

\subsubsection{Higher-level parameters as variables}

Strategy is a vast subject and impacts tactics very much. There are several ``strategic dimensions'', but we can stick to the two strategy axes: aggressiveness (initiative), and economy/technology/production balance. 
These dimensions are tightly coupled inside a strategical plan, as putting all our resources in economy at time $t$ is not aimed at attacking at time $t+1$. Likewise, putting all our resources in military production at time $t$ is a mistake if we do not intend to be aggressive at time $t+1$. However, there are several combinations of values along these dimensions, which lead to different strategies. We detailed these higher order views of the strategy in section~\ref{sec:whatisstrategy}. 

In our models (tactics \& army composition), the aggressiveness is a parameter. Instead these two strategic dimensions could be encoded in variables:
\begin{itemize}
    \item $A \in \{true,false\}$ for the aggressiveness/initiative, which will influence tactical (When do we attack?) and strategic models (How much do we adapt and how much do we follow our initiative? How do we adapt?).
    \item $ETP \in \{eco,tech,prod\}$ which will arbitrate how we balance our resources between economic expansion, technology advances and units production.
\end{itemize}
A possibility is yet again to try and use a multi-armed bandit acting on this two variables ($A$ and $ETP$), which (hyper-)parametrize all subsequent models. At this level, there is a lot of context to be taken into account, and so we should specifically consider contextual bandits.

Another interesting perspective would be to use a hierarchy (from strategy to tactics) of reinforcement learning of (all) the bot's parameters, which can be seen as learning the degrees of liberty (of the whole bot) one by one as in \citep{baranes2009}. A more realistic task is to learn only these parameters that we cannot easily learn from datasets or correspond to abstract strategical and tactical thinking (like $A$ and $ETP$).

\subsubsection{Production \& construction planning}

A part of strategy that we did not study here is production planning, it encompasses planning the use (and future collection) of resources, the construction of buildings and the production of units. Our bot uses a simple search, some other bots have more optimized build-order search, for instance UAlbertaBot uses a build-order abstraction with depth-first branch and bound \citep{Churchill2011}. Planning can be considered (and have been, for instance \citep{BartheyeJ09}), but it needs to be efficient enough to re-plan often. Also, it could be interesting to interface the plan with the uncertainty about the opponent's state (Will we be attacked in the next minute? What is the opponent's technology?). 

A naive probabilistic planning approach would be to plan short sequences of constructions as ``bricks'' of a full production plan, and assign them probability variables which will depend on the beliefs on the opponent's states, and replan online depending on the bricks probabilities. For instance, if we are in a state with a Protoss Cybernetics Core (and other lower technology buildings), we may have a sequence ``Protoss Robotics Facility $\rightarrow$ Protoss Observatory'' (which is not mutually exclusive to a sequence with Protoss Robotics Facility only). This sequence unlocks detector technology (Observers are detectors, produced out of the Robotics Facility), so its probability variable ($RO$) should be conditioned on the belief that the opponent has invisible units ($\PP(RO=true|Opening=DarkTemplar)=high$ or parametrize $\PP(RO|ETT)$ adequately). The planner would both use resources planning, time of construction, and this posterior probabilities on sequences.
%\citep{PlanningUnderUncertainty11}
%Planning \citep{Wolfe11}\\
%Bandits (contextual bandits)\\

\subsection{Inter-game Adaptation (Meta-game)}


In a given match, and/or against a given player, players tend to learn from their immediate mistakes, and they adapt their strategies to each other's. As this can be seen as a continuous learning problem (reinforcement learning, exploration-exploitation trade-off), there is more to it. Human players call this the \textit{\glos{metagame}}, as they enter the ``I think that he thinks that I think...'' game until arriving at fixed points. This ``meta-game'' is closely related to the balance of the game, and the fact that there are several equilibriums makes the interest of StarCraft. Also, clearly, for human players there is psychology involved. 

\subsubsection{Continuous learning}

For all strategic models, the possible improvements (subsections~\ref{sec:buildtreeimprovements}, \ref{sec:openingspossibleimprovements}, \ref{sec:armycompoextensions}) would include to learn specific sets of parameters against the opponent's strategies. The problem here is that (contrary to battles/tactics) there are not much observations (games against a given opponent) to learn from. For instance, a naive approach would be to learn a Laplace's law of succession directly on $P(ETechTrees=ett|Player=p)= \frac{1 + nbgames(ett,p)}{\#ETT + nbgames(p)}$, and do the same for $EClusters$, but this could require several games. Even if we see more than one tech tree per game for the opponent, a few games will still only show a sparse subset of $ETT$. Another part of this problem could arise if we want to learn really in-game as we would only have partial observations. %(and $ETactics$),  

\cite{metalevelbehavioradaptrts} approached ``meta-level behavior adaptation in RTS games'' as a mean for their case-based planning AI to learn from its own failures in Wargus. The opponent is not considered at all, but this could be an interesting entry point to discover bugs or flaws in the bot's parameters.

\subsubsection{Bot's psychological warfare}

Our main idea for real meta-game playing by our AI would be to use our models recursively. As for some of our models (tactics \& strategy), that can be used both for prediction and decision-making, we could have a full model of the enemy by maintaining the state of a model from them, with their inputs (and continually learn some of the parameters). For instance, if we have our army adaptation model for ourselves and for the opponent, we need to incorporate the output of their model as an input of our model, in the part which predicts the opponent's future army composition. If we cycle (iterate) the reasoning (``I will produce this army because they will have this one''...), we should reach these meta-game equilibriums.


\subsubsection{Final words}

Finally, bots are as far from having a psychological model of their opponent as from beating the best human players. I believe that this is our adaptability, our continuous learning, which allows human players (even simply ``good'' ones like me) to beat RTS games bots consistently. When robotic AI will start winning against human players, we may want to hinder them to have only partial vision of the world (as humans do through a screen) and a limited number of concurrent actions (humans use a keyboard and a mouse so they have limited \glos{APM}). At this point, they will need an attention model and some form of hierarchical action selection. Before that, all the problems arose in this thesis should be solved, at least at the scale of the RTS domain.




%\citep{RussellW89}


